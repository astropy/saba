import numpy as np
from collections import OrderedDict
from sherpa.fit import Fit
from sherpa.data import Data1D, Data1DInt, Data2D, Data2DInt, DataSimulFit
from sherpa.data import BaseData
from sherpa.models import UserModel, Parameter, SimulFitModel
from astropy.modeling.fitting import Fitter
from astropy.utils.exceptions import AstropyUserWarning
from sherpa.stats import Chi2, Chi2ConstVar, Chi2DataVar, Chi2Gehrels
from sherpa.stats import Chi2ModVar, Chi2XspecVar, LeastSq
from sherpa.stats import CStat, WStat, Cash
from sherpa.optmethods import GridSearch, LevMar, MonCar, NelderMead
from sherpa.estmethods import Confidence, Covariance, Projection
from sherpa.sim import MCMC
import inspect
import types
# from astropy.modeling

__all__ = ('SherpaFitter', 'SherpaMCMC')


class SherpaWrapper(object):
    value = None

    def __init__(self, value=None):
        if value is not None:
            self.set(value)

    def set(self, value):
        try:
            self.value = self._sherpa_values[value.lower()]
        except KeyError:
            UserWarning("Value not found")  # todo handle


class Stat(SherpaWrapper):

    """
    A wrapper for the fit statistics of sherpa

    Parameters
    ----------
        value: String
            the name of a sherpa statistics.
    """

    _sherpa_values = {'cash': Cash, 'wstat': WStat, 'cstat': CStat,
                      'chi2': Chi2, 'chi2constvar': Chi2ConstVar,
                      'chi2datavar': Chi2DataVar,
                      'chi2gehrels': Chi2Gehrels,
                      'chi2modvar': Chi2ModVar,
                      'chi2xspecvar': Chi2XspecVar,
                      'leastsq': LeastSq}


class OptMethod(SherpaWrapper):

    """
    A wrapper for the optimization methods of sherpa

    Parameters
    ----------
        value: String
            the name of a sherpa optimization method.
    """
    _sherpa_values = {'simplex': GridSearch, 'levmar': LevMar,
                      'moncar': MonCar, 'neldermead': NelderMead}


class EstMethod(SherpaWrapper):

    """
    A wrapper for the error estimation methods of sherpa

        Parameters
        ----------
        value: String
            the name of a sherpa statistics.
    """

    _sherpa_values = {'confidence': Confidence, 'covariance': Covariance,
                      'projection': Projection}




class SherpaMCMC(object):
    """
        An interface which makes use of sherpa's MCMC(pyBLoCXS) functionality.

        Parameters
        ----------
        fitter: a `SherpaFitter` instance:
                used to caluate the fit statstics, must have been fit as
                the covariance matrix is used.
        smapler: string
                the name of a valid sherpa sampler

        walker: string
                the name of a valid sherpa walker

    """

    def __init__(self, fitter, sampler='mh', walker='mh'):
        self._mcmc = MCMC()

        if hasattr(fitter.fit_info, "statval"):
            self._fitter = fitter._fitter

            if hasattr(fitter.error_info, "extra_output"):
                fitter.est_errors()
            self._cmatrix = fitter.errors_info.extra_output
            pars = fitter._fitmodel.sherpa_model.pars
            self.parameter_map = OrderedDict(map(lambda x: (x.name, x),
                                             pars))
        else:
            raise AstropyUserWarning("Must have valid fit! "
                                     "Convariance matrix is not present")

    def __call__(self, niter=200000):
        draws = self._mcmc.get_draws(self._fitter, self._cmatrix,
                                     niter=niter)
        self._stat_vals, self._accepted, self._parameter_vals = draws
        self.acception_rate = (self._accepted.sum() * 100.0 /
                               self._accepted.size)
        self.parameters = OrderedDict()
        for n, parameter_set in enumerate(self._parameter_vals):
            pname = self.parameter_map.keys()[n]
            self.parameters[pname] = self._parameter_vals[n, :]
        return draws

    def set_sampler_options(self, opt, value):
        """
        Set an option for the current MCMC sampler.

        Parameters
        ----------
        opt : str
           The option to change. Use `get_sampler` to view the
           available options for the current sampler.
        value
           The value for the option.

        Notes
        -----
        The options depend on the sampler. The options include:

        defaultprior
           Set to ``False`` when the default prior (flat, between the
           parameter's soft limits) should not be used. Use
           `set_prior` to set the form of the prior for each
           parameter.

        inv
           A bool, or array of bools, to indicate which parameter is
           on the inverse scale.

        log
           A bool, or array of bools, to indicate which parameter is
           on the logarithm (natural log) scale.

        original
           A bool, or array of bools, to indicate which parameter is
           on the original scale.

        p_M
           The proportion of jumps generatd by the Metropolis
           jumping rule.

        priorshape
           An array of bools indicating which parameters have a
           user-defined prior functions set with `set_prior`.

        scale
           Multiply the output of `covar` by this factor and
           use the result as the scale of the t-distribution.

        Examples
        --------
        >> mcmc = SherpaMCMC(sfit)
        >> mcmc.set_sampler_opt('scale', 3)
        """
        self._mcmc.set_sampler_opt(opt, value)

    def get_sampler(self,):
        return self._mcmc.get_sampler()

    def set_prior(self, parameter, prior):
        """
        Set the prior function to use with a parameter.

        The default prior used by ``get_draws`` for each parameter
        is flat, varying between the hard minimum and maximum
        values of the parameter (as given by the ``hard_min`` and
        ``hard_max`` attributes of the parameter object). The ``set_prior``
        function is used to change the form of the prior for a
        parameter.

        Parameters
        ----------
        par : sherpa.models.parameter.Parameter instance
           A parameter of a model instance.
        prior : function or sherpa.models.model.Model instance
           The function to use for a prior. It must accept a
           single argument and return a value of the same size
           as the input.

        Examples
        --------

        Create a function (``lognorm``) and use it as the prior the
        ``nH`` parameter:
            >> def lognorm(x):
               # center on 10^20 cm^2 with a sigma of 0.5
               sigma = 0.5
               x0 = 20
               # nH is in units of 10^-22 so convert
               dx = np.log10(x) + 22 - x0
               norm = sigma / np.sqrt(2 * np.pi)
               return norm * np.exp(-0.5*dx*dx/(sigma*sigma))

            >> mcmc.set_prior('nH', lognorm)
        """
        if parameter in self.parameter_map:
            self._mcmc.set_prior(self.parameter_map[parameter], prior)
        else:
            raise AstropyUserWarning("Parmater {name} not found in parameter"
                                     "map".format(name=parameter))


class doc_wrapper(object):

    def __init__(self, f, pre="", post=""):
        self._f = f
        self._pre = pre
        self._post = post

    @property
    def __doc__(self,):
        return "".join([self._pre, self._f.__doc__, self._post])

    def __call__(self, instance, *args, **kwargs):
        return self._f(instance, *args, **kwargs)

    def __get__(self, instance, cls=None):
        ''' This implements the descriptor protocol and allows this
        callable class to be used as a bound method.
        See:
        https://docs.python.org/2/howto/descriptor.html#functions-and-methods
        '''
        return types.MethodType(self, instance, cls)


class SherpaFitter(Fitter):
    __doc__ = """
    Sherpa Fitter for astropy models. Yay :)

    Parameters
    ----------
        optimizer : string
            the name of a sherpa optimizer.
            posible options include:
                {opt}
        statistic : string
            the name of a sherpa statistic.
            posible options include:
                {stat}
        estmethod : string
            the name of a sherpa estmethod.
            posible options include:
                {est}
    """.format(opt=", ".join(OptMethod._sherpa_values.keys()),
               stat=", ".join(Stat._sherpa_values.keys()),
               est=", ".join(EstMethod._sherpa_values.keys()))  # is this evil?

    def __init__(self, optimizer="levmar", statistic="leastsq", estmethod="covariance"):
        try:
            optimizer = optimizer.value
        except AttributeError:
            optimizer = OptMethod(optimizer).value

        try:
            statistic = statistic.value
        except AttributeError:
            statistic = Stat(statistic).value

        super(SherpaFitter, self).__init__(optimizer=optimizer, statistic=statistic)

        try:
            self._est_method = estmethod.value()
        except AttributeError:
            self._est_method = EstMethod(estmethod).value()

        self.fit_info = {}
        self._fitter = None  # a handle for sherpa fit function
        self._fitmodel = None  # a handle for sherpa fit model
        self._data = None  # a handle for sherpa dataset
        self.error_info = {}

    get_sampler = doc_wrapper(SherpaMCMC, "This returns and instance of `SherpaMCMC` with it's self as the fitter:\n")


    def __call__(self, models, x, y, z=None, xbinsize=None, ybinsize=None, err=None, bkg=None, bkg_scale=1, **kwargs):
        """
        Fit the astropy model with a the sherpa fit routines.

        Parameters
        ----------
            models : `astropy.modeling.FittableModel` or list of `astropy.modeling.FittableModel`
                model to fit to x, y, z
            x : array or list of arrays
                input coordinates
            y : array or list of arrays
                input coordinates
            z : array or list of arrays (optional)
                input coordinates
            xbinsize : array or list of arrays (optional)
                an array of xbinsizes in x  - this will be x -/+ (binsize  / 2.0)
            ybinsize : array or list of arrays (optional)
                an array of xbinsizes in y  - this will be y -/+ (ybinsize / 2.0)
            err : array or list of arrays (optional)
                an array of errors in dependant variable
            bkg : array or list of arrays (optional)
                this will act as background data
            bkg_sale : float or list of floats (optional)
                the scaling factor for the dataset if a single value
                is supplied it will be copied for eash dataset
            **kwargs:
                keyword arguments will be passed on to sherpa fit routine

        Returns
        -------
            model_copy : `astropy.modeling.FittableModel` or a list of models.
                a copy of the input model with parameters set by the fitter
        """

        tie_list = []
        try:
            n_inputs = models[0].n_inputs
        except TypeError:
            n_inputs = models.n_inputs


        self._data = Dataset(n_inputs, x, y, z, xbinsize, ybinsize, err, bkg, bkg_scale)

        if self._data.ndata > 1:
            if len(models) == 1:
                self._fitmodel = ConvertedModel([models.copy() for _ in xrange(self._data.ndata)], tie_list)
                # Copy the model so each data set has the same model!
            elif len(models) == self._data.ndata:
                self._fitmodel = ConvertedModel(models, tie_list)
            else:
                raise Exception("Don't know how to handle multiple models unless there is one foreach dataset")
        else:
            if len(models) > 1:
                self._data.make_simfit(len(models))
                self._fitmodel = ConvertedModel(models, tie_list)
            else:
                self._fitmodel = ConvertedModel(models)

        self._fitter = Fit(self._data.data, self._fitmodel.sherpa_model, self._stat_method, self._opt_method, self._est_method, **kwargs)
        self.fit_info = self._fitter.fit()

        return self._fitmodel.get_astropy_model()

    def est_errors(self, sigma=None, maxiters=None, numcores=1, methoddict=None, parlist=None):
        """
        Use sherpa error estimators based on the last fit.

        Parameters
        ----------
            sigma: float
                this will be set as the confidance interval for which the errors are found too.
            maxiters: int
                the maximum number of iterations the error estimator will run before giving up.
            methoddict: dict
                !not quite sure couldn't figure this one out yet!
            parlist: list
                a list of parameters to find the confidance interval of if none are provided all free
                parameters will be estimated.
        """
        if self._fitter is None:
            ValueError("Must complete a valid fit before errors can be calculated")
        if sigma is not None:
            self._fitter.estmethod.config['sigma'] = sigma
        if maxiters is not None:
            self._fitter.estmethod.config['maxiters'] = maxiters
        if 'numcores' in self._fitter.estmethod.config:
            if not numcores == self._fitter.estmethod.config['numcores']:
                self._fitter.estmethod.config['numcores'] = numcores

        self.error_info = self._fitter.est_errors(methoddict=methoddict, parlist=parlist)
        pnames = [p.split(".", 1)[-1] for p in self.error_info.parnames]  # this is to remove the model name
        return pnames, self.error_info.parvals, self.error_info.parmins, self.error_info.parmaxes


class Dataset(SherpaWrapper):

    """
    Parameters
    ----------
        n_dim: int
            Used to veirfy required number of dimentions.
        x : array (or list of arrays)
            input coordinates
        y : array (or list of arrays)
            input coordinates
        z : array (or list of arrays) (optional)
            input coordinates
        xbinsize : array (or list of arrays) (optional)
            an array of errors in x
        ybinsize : array (or list of arrays) (optional)
            an array of errors in y
        err : array (or list of arrays) (optional)
            an array of errors in z
        bkg : array or list of arrays (optional)
                this will act as background data
        bkg_sale : float or list of floats (optional)
                the scaling factor for the dataset if a single value
                is supplied it will be copied for eash dataset
    returns:
        _data: a sherpa dataset
    """

    def __init__(self, n_dim, x, y, z=None, xbinsize=None, ybinsize=None, err=None, bkg=None, bkg_scale=1):

        x = np.array(x)
        y = np.array(y)
        if x.ndim == 2 or (x.dtype == np.object or y.dtype == np.object):
            data = []
            if z is None:
                z = len(x) * [None]

            if xbinsize is None:
                xbinsize = len(x) * [None]

            if ybinsize is None:
                ybinsize = len(y) * [None]

            if err is None:
                err = len(z) * [None]

            if bkg is None:
                bkg = len(x) * [None]
            try:
                iter(bkg_scale)
            except TypeError:
                bkg_scale = len(x) * [bkg_scale]


            for nn, (xx, yy, zz, xxe, yye, zze, bkg, bkg_scale) in enumerate(zip(x, y, z, xbinsize, ybinsize, err, bkg, bkg_scale)):
                data.append(self._make_dataset(n_dim, x=xx, y=yy, z=zz, xbinsize=xxe, ybinsize=yye, err=zze, bkg=bkg, bkg_scale=bkg_scale, n=nn))
            self.data = DataSimulFit("wrapped_data", data)
            self.ndata = nn + 1
        else:
            self.data = self._make_dataset(n_dim, x=x, y=y, z=z, xbinsize=xbinsize, ybinsize=ybinsize, err=err, bkg=bkg, bkg_scale=bkg_scale)
            self.ndata = 1

    @staticmethod
    def _make_dataset(n_dim, x, y, z=None, xbinsize=None, ybinsize=None, err=None, bkg=None, bkg_scale=1, n=0):
        """
        Parameters
        ----------
            n_dim: int
                Used to veirfy required number of dimentions.
            x : array
                input coordinates
            y : array
                input coordinates
            z : array (optional)
                input coordinatesbkg
            xbinsize : array (optional)
                an array of errors in x
            ybinsize : array (optional)
                an array of errors in y
            err : array (optional)
                an array of errors in z
            n  : int
                used in error reporting

        returns:
            _data: a sherpa dataset
        """

        if (z is None and n_dim > 1) or (z is not None and n_dim == 1):
            raise ValueError("Model and data dimentions don't match in dataset %i" % n)

        if z is None:
            assert x.shape == y.shape, "shape of x and y don't match in dataset %i" % n
        else:
            z = np.asarray(z)
            assert x.shape == y.shape == z.shape, "shapes x,y and z don't match in dataset %i" % n

        if xbinsize is not None:
            xbinsize = np.array(xbinsize)
            assert x.shape == xbinsize.shape, "x's and xbinsize's shapes do not match in dataset %i" % n

        if z is not None and err is not None:
            err = np.array(err)
            assert z.shape == err.shape, "z's and err's shapes do not match in dataset %i" % n

            if ybinsize is not None:
                ybinsize = np.array(ybinsize)
                assert y.shape == ybinsize.shape, "y's and ybinsize's shapes do not match in dataset %i" % n

        else:
            if err is not None:
                err = np.array(err)
                assert y.shape == err.shape, "y's and err's shapes do not match in dataset %i" % n

        if xbinsize is not None:
            
            bs = xbinsize / 2.0

        if z is None:
            if xbinsize is None:
                if err is None:
                    if bkg is None:
                        data = Data1D("wrapped_data", x=x, y=y)
                    else:
                        data = Data1DBkg("wrapped_data", x=x, y=y, bkg=bkg, bkg_scale=bkg_scale)
                else:
                    if bkg is None:
                        data = Data1D("wrapped_data", x=x, y=y, staterror=err)
                    else:
                        data = Data1DBkg("wrapped_data", x=x, y=y, staterror=err, bkg=bkg, bkg_scale=bkg_scale)
            else:
                if err is None:
                    if bkg is None:
                        
                        data = Data1DInt("wrapped_data", xlo=x - bs, xhi=x + bs, y=y)
                    else:
                        data = Data1DIntBkg("wrapped_data", xlo=x - bs, xhi=x + bs, y=y, bkg=bkg, bkg_scale=bkg_scale)
                else:
                    if bkg is None:
                        data = Data1DInt("wrapped_data", xlo=x - bs, xhi=x + bs, y=y, staterror=err)
                    else:
                        data = Data1DIntBkg("wrapped_data", xlo=x - bs, xhi=x + bs, y=y, staterror=err, bkg=bkg, bkg_scale=bkg_scale)
        else:
            if xbinsize is None and ybinsize is None:
                if err is None:
                    if bkg is None:
                        data = Data2D("wrapped_data", x0=x, x1=y, y=z)
                    else:
                        data = Data2DBkg("wrapped_data", x0=x, x1=y, y=z, bkg=bkg, bkg_scale=bkg_scale)
                else:
                    if bkg is None:
                        data = Data2D("wrapped_data", x0=x, x1=y, y=z, staterror=err)
                    else:
                        data = Data2DBkg("wrapped_data", x0=x, x1=y, y=z, staterror=err, bkg=bkg, bkg_scale=bkg_scale)
            elif xbinsize is not None and ybinsize is not None:
                ys = ybinsize / 2.0
                if err is None:
                    if bkg is None:
                        data = Data2DInt("wrapped_data", x0lo=x - bs, x0hi=x + bs, x1lo=y - ys, x1hi=y + ys, y=z)
                    else:
                        data = Data2DIntBkg("wrapped_data", x0lo=x - bs, x0hi=x + bs, x1lo=y - ys, x1hi=y + ys, y=z, bkg=bkg, bkg_scale=bkg_scale)
                else:
                    if bkg is None:
                        data = Data2DInt("wrapped_data", x0lo=x - bs, x0hi=x + bs, x1lo=y - ys, x1hi=y + ys, y=z, staterror=err)
                    else:
                        data = Data2DIntBkg("wrapped_data", x0lo=x - bs, x0hi=x + bs, x1lo=y - ys, x1hi=y + ys, y=z, staterror=err, bkg=bkg, bkg_scale=bkg_scale)
            else:
                raise ValueError("Set xbinsize and ybinsize, or set neither!")

        return data

    def make_simfit(self, numdata):
        """
        This makes a single datasets into a simdatafit at allow fitting of multiple models by copying the single dataset!

        Parameters
        ----------
            numdata: int
                the number of times you want to copy the dataset i.e if you want 2 datasets total you put 1!
        """

        self.data = DataSimulFit("wrapped_data", [self.data for _ in xrange(numdata)])
        self.ndata = numdata + 1


class ConvertedModel(object):

    """
    This  wraps the model convertion to sherpa models and from astropy models and back!

    Parameters
        ----------
        models: model : `astropy.modeling.FittableModel` (or list of)

        tie_list: list (optional)
            a list of parameter pairs which will be tied accross models
            e.g. [(modelB.y, modelA.x)] will mean that y in modelB will be tied to x of modelA
    """

    def __init__(self, models, tie_list=None):
        self.model_dict = OrderedDict()
        try:
            models.parameters  # does it quack
            self.sherpa_model = self._astropy_to_sherpa_model(models)
            self.model_dict[models] = self.sherpa_model
        except AttributeError:
            for mod in models:
                self.model_dict[mod] = self._astropy_to_sherpa_model(mod)
                if tie_list is not None:
                    for par1, par2 in tie_list:
                        getattr(self.model_dict[par1._model], par1.name).link = getattr(self.model_dict[par2._model], par2.name)
            self.sherpa_model = SimulFitModel("wrapped_fit_model", self.model_dict.values())

    @staticmethod
    def _astropy_to_sherpa_model(model):
        """
        Converts the model using sherpa's usermodel suppling the parameter detail to sherpa
        then using a decorator to allow the call method to act like the calc method
        """
        def _calc2call(func):
            """This decorator makes call and calc work together."""
            def _converter(inp, *x):
                if func.n_inputs == 1:
                    retvals = func.evaluate(x[0], *inp)
                else:
                    retvals = func.evaluate(x[0], x[1], *inp)
                return retvals
            return _converter

        if len(model.ineqcons) > 0 or len(model.eqcons) > 0:
            AstropyUserWarning('In/eqcons are not supported by sherpa these will be ignored!')

        pars = []
        linkedpars = []
        for pname in model.param_names:
            param = getattr(model, pname)
            vals = [param.name, param.value, param.min, param.max, param.min,
                    param.max, None, param.fixed, False]
            attrnames = ["name", "val", "min", "max", "hard_min", "hard_max",
                         "units", "frozen", "alwaysfrozen"]
            if model.name is None:
                model._name = ""

            pars.append(Parameter(modelname="wrap_" + model.name, **dict([(atr, val) for atr, val in zip(attrnames, vals) if val is not None])))
            if param.tied is not False:
                linkedpars.append(pname)

        smodel = UserModel(model.name, pars)
        smodel.calc = _calc2call(model)

        for pname in linkedpars:
            param = getattr(model, pname)
            sparam = getattr(smodel, pname)
            sparam.link = param.tied(smodel)

        return smodel

    def get_astropy_model(self):
        """Returns an astropy model based on the sherpa model"""
        return_models = []

        for apymod, shmod in self.model_dict.items():
            return_models.append(apymod.copy())
            for pname, pval in map(lambda p: (p.name, p.val), shmod.pars):
                getattr(return_models[-1], pname.split(".")[-1]).value = pval

        if len(return_models) > 1:
            return return_models
        else:
            return return_models[0]


class Data1DIntBkg(Data1DInt):
    """
       Data1DInt which tricks sherpa into using the background object without using DataPHA
       Parameters
       ----------
            name: string
                dataset name

            xlo: array
               the array which represents the lower x value for the x bins

            xhi: array
               the array which represents the upper x value for the x bins

            y: array
               the array which represents y data

            bkg: array
               the array which represents bkgdata

            staterror: array (optional)
                the array which represents the errors on z

            bkg_scale: float
                the scaling factor for background data

            src_scale: float
                the scaling factor for source data
    """

    _response_ids = [0]
    _background_ids = [0]

    @property
    def response_ids(self):
        return self._response_ids

    @property
    def background_ids(self):
        return self._background_ids

    @property
    def backscal(self):
        return self._bkg_scale

    def get_background(self, index):
        return self._backgrounds[index]

    def __init__(self, name, xlo, xhi, y, bkg, staterror=None, bkg_scale=1, src_scale=1):
        self._bkg = np.asanyarray(bkg)
        self._bkg_scale = src_scale
        self.exposure = 1

        self.subtracted = False

        self._backgrounds = [BkgDataset(bkg, bkg_scale)]
        BaseData.__init__(self)

        self.xlo = xlo
        self.xhi = xhi
        self.y = y
        self.staterror = staterror


class Data1DBkg(Data1D):
    """
       Data1D which tricks sherpa into using the background object without using DataPHA
        Parameters
        ----------
            name: string
                dataset name

            x: array
               the array which represents the x values

            y: array
               the array which represents y data

            bkg: array
               the array which represents background data

            staterror: array (optional)
                the array which represents the errors on z

            bkg_scale: float
                the scaling factor for background data

            src_scale: float
                the scaling factor for source data
    """

    _response_ids = [0]
    _background_ids = [0]

    @property
    def response_ids(self):
        return self._response_ids

    @property
    def background_ids(self):
        return self._background_ids

    @property
    def backscal(self):
        return self._bkg_scale

    def get_background(self, index):
        return self._backgrounds[index]

    def __init__(self, name, x, y, bkg, staterror=None, bkg_scale=1, src_scale=1):
        self._bkg = np.asanyarray(bkg)
        self._bkg_scale = src_scale
        self.exposure = 1
        self.subtracted = False

        self._backgrounds = [BkgDataset(bkg, bkg_scale)]
        BaseData.__init__(self)

        self.x = x
        self.y = y
        self.staterror = staterror


class Data2DIntBkg(Data2DInt):
    """
       Data2DInt which tricks sherpa into using the background object without using DataPHA
        Parameters
        ----------
            name: string
                dataset name

            xlo: array
               the array which represents the lower x value for the x bins

            xhi: array
               the array which represents the upper x value for the x bins

            ylo: array
               the array which represents the lower y value for the y bins

            yhi: array
               the array which represents the upper y value for the y bins


            z: array
               the array which represents z data

            bkg: array
               the array which represents bkgdata

            staterror: array (optional)
                the array which represents the errors on z

            bkg_scale: float
                the scaling factor for background data

            src_scale: float
                the scaling factor for source data
    """

    _response_ids = [0]
    _background_ids = [0]

    @property
    def response_ids(self):
        return self._response_ids

    @property
    def background_ids(self):
        return self._background_ids

    @property
    def backscal(self):
        return self._bkg_scale

    def get_background(self, index):
        return self._backgrounds[index]

    def __init__(self, name, xlo, xhi, ylo, yhi, z, bkg, staterror=None, bkg_scale=1, src_scale=1):
        self._bkg = np.asanyarray(bkg)
        self._bkg_scale = src_scale
        self.exposure = 1

        self.subtracted = False

        self._backgrounds = [BkgDataset(bkg, bkg_scale)]
        BaseData.__init__(self)

        self.xlo = xlo
        self.xhi = xhi
        self.ylo = ylo
        self.yhi = yhi
        self.z = z
        self.staterror = staterror


class Data2DBkg(Data2D):
    """
       Data2D which tricks sherpa into using the background object without
       using DataPHA
       Parameters
       ----------
           name: string
                dataset name

            x: array
               the array which represents x data

            y: array
               the array which represents y data

            z: array
               the array which represents z data

            bkg: array
               the array which represents bkgdata

            staterror: array (optional)
                the array which represents the errors on z

            bkg_scale: float
                the scaling factor for background data

            src_scale: float
                the scaling factor for source data
    """

    _response_ids = [0]
    _background_ids = [0]

    @property
    def response_ids(self):
        return self._response_ids

    @property
    def background_ids(self):
        return self._background_ids

    @property
    def backscal(self):
        return self._bkg_scale

    def get_background(self, index):
        return self._backgrounds[index]

    def __init__(self, name, x, y, z, bkg, staterror=None, bkg_scale=1, src_scale=1):
        self._bkg = np.asanyarray(bkg)
        self._bkg_scale = src_scale
        self.exposure = 1
        self.subtracted = False

        self._backgrounds = [BkgDataset(bkg, bkg_scale)]
        BaseData.__init__(self)

        self.x = x
        self.y = y
        self.z = z
        self.staterror = staterror


class BkgDataset(object):
    """
        The background object which is used to caclulate fit
        stat's which require it.
        Parameters
        ----------
            bkg: array
                the background data
            bkg_scale: float
                the ratio of src/bkg
    """

    def __init__(self, bkg, bkg_scale):
        self._bkg = bkg
        self._bkg_scale = bkg_scale
        self.exposure = 1

    def get_dep(self, flag):
        return self._bkg

    @property
    def backscal(self):
        return self._bkg_scale
